{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(df, K=2):\n",
    "    nans = df.isnull().values.astype(int)\n",
    "\n",
    "    # Clustering on lines\n",
    "    kmeans = KMeans(n_clusters=K).fit(nans)\n",
    "\n",
    "    # Get datasets & corresponding indexes\n",
    "    list_df = []\n",
    "    list_ind = []\n",
    "\n",
    "    for k in range(K):\n",
    "        ind = np.array(kmeans.labels_ == k)\n",
    "        list_df.append(df[ind])\n",
    "        list_ind.append(ind)\n",
    "\n",
    "    return list_df, list_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataframes(df):\n",
    "    dfs, idxs = split_dataset(df)\n",
    "    df_1, df_2, idx_1, idx_2 = dfs[0], dfs[1], idxs[0], idxs[1]\n",
    "\n",
    "    # Check which one contains most NaNs\n",
    "    if any([all(c) for c in df_1.isnull().values.T]):\n",
    "        df_nan, idx_nan, df_full, idx_full = df_1, idx_1, df_2, idx_2\n",
    "    else:\n",
    "        df_nan, idx_nan, df_full, idx_full = df_2, idx_2, df_1, idx_1\n",
    "\n",
    "    return df_full, df_nan, idx_full, idx_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_na(df, threshold=0.95):\n",
    "    # Compute NaN rate per column\n",
    "    nan_rates = df.isnull().values.astype(np.float32).mean(axis=0)\n",
    "    # Drop columns with NaN rate greater than threshold\n",
    "    to_drop = [c for (i, c) in enumerate(df) if nan_rates[i] > threshold]\n",
    "    df = df.drop(to_drop, axis=1, inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        Columns of dtype object are imputed with the most frequent value in column.\n",
    "        Columns of other types are imputed with mean of column.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dummyfier(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"Transform categorical features into one-hot encoded columns.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Find categorical features\n",
    "        cat_cols = [c for c in X.columns if X[c].dtype == np.dtype('O')]\n",
    "        self.features_ = dict(zip(cat_cols, [X[c].unique() for c in cat_cols]))\n",
    "        \n",
    "        # Deal with too numerous values feature (e.g. v22)\n",
    "        self.to_drop = []\n",
    "        for c, values in self.features_.items():\n",
    "            if values.shape[0] > 1000:\n",
    "                self.to_drop.append(c)\n",
    "        for c in self.to_drop:\n",
    "            del self.features_[c]\n",
    "        \n",
    "        # Fit one LabelBinarizer per categorical feature\n",
    "        self.binarizers_ = {}\n",
    "        for c in self.features_:\n",
    "            self.binarizers_[c] = LabelBinarizer(sparse_output=False).fit(X[c])\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Compute one-hot encoded matrix for each categorical feature\n",
    "        X_new = []\n",
    "        for c, b in self.binarizers_.items():\n",
    "            X_b = pd.DataFrame(b.transform(X[c]), columns=[\"%s_%s\" % (c, v) for v in b.classes_], index=X.index)\n",
    "            X_new.append(X_b)\n",
    "        \n",
    "        # Drop categorical features\n",
    "        X_ = X.drop(list(self.features_.keys()) + self.to_drop, axis=1)\n",
    "        \n",
    "        return pd.concat([X_] + X_new, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xy_split(df, target='target'):\n",
    "    return df.drop([target], axis=1), df[target]\n",
    "\n",
    "\n",
    "def preprocess(X, y=None):\n",
    "    X_full, X_nan, idx_full, idx_nan = prepare_dataframes(X)\n",
    "    X_full = drop_na(X_full)\n",
    "    X_nan = drop_na(X_nan)\n",
    "    if y is not None:\n",
    "        y_full = y[idx_full]\n",
    "        y_nan = y[idx_nan]\n",
    "    else:\n",
    "        y_full, y_nan = None, None\n",
    "    return X_full, X_nan, y_full, y_nan, idx_full, idx_nan\n",
    "\n",
    "\n",
    "def reconstruct(y_list, idx_list):\n",
    "    p = y_list[0].ndim\n",
    "    assert all([y.ndim == p for y in y_list]), \"Arrays in list must have the same ndim.\"\n",
    "    \n",
    "    n = sum([y.shape[0] for y in y_list])\n",
    "    if p == 1:\n",
    "        out = np.zeros((n, ))\n",
    "        for idx, y in zip(idx_list, y_list):\n",
    "            out[idx] = y\n",
    "        return out\n",
    "    else:\n",
    "        p = y_list[0].shape[1]\n",
    "        out = np.zeros((n, p))\n",
    "        for idx, y in zip(idx_list, y_list):\n",
    "            out[idx, :] = y\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', index_col='ID')\n",
    "print(\"Loading dataset... ok\")\n",
    "\n",
    "X, y = xy_split(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=57)\n",
    "print(\"Creating train and validation datasets... ok\")\n",
    "\n",
    "# Split dataset into to 2 dataframes (one with many NaNs, another with few NaNs)\n",
    "X_full, X_nan, y_full, y_nan, idx_full, idx_nan = preprocess(X_train, y_train)\n",
    "print(\"Splitting dataset... ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_full = Pipeline([\n",
    "    ('imputer', DataFrameImputer()),\n",
    "    ('dummyfier', Dummyfier()),\n",
    "    ('pca', PCA(n_components=25, svd_solver='arpack')),\n",
    "    ('clf', RandomForestClassifier(n_estimators=150, max_depth=6, n_jobs=3)),\n",
    "])\n",
    "pipe_nan = Pipeline([\n",
    "    ('imputer', DataFrameImputer()),\n",
    "    ('dummyfier', Dummyfier()),\n",
    "    ('pca', PCA(n_components=25, svd_solver='arpack')),\n",
    "    ('clf', RandomForestClassifier(n_estimators=150, max_depth=6, n_jobs=3)),\n",
    "])\n",
    "\n",
    "pipe_full.fit(X_full, y_full)\n",
    "print(\"Fitting Full Model... ok\")\n",
    "pipe_nan.fit(X_nan, y_nan)\n",
    "print(\"Fitting NaN Model... ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validation set predictions\n",
    "Xt_full, Xt_nan, yt_full, yt_nan, idxt_full, idxt_nan = preprocess(X_test, y_test)\n",
    "preds_full = pipe_full.predict(Xt_full)\n",
    "probas_full = pipe_full.predict_proba(Xt_full)\n",
    "preds_nan = pipe_nan.predict(Xt_nan)\n",
    "probas_nan = pipe_nan.predict_proba(Xt_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = reconstruct([preds_full, preds_nan], [idxt_full, idxt_nan])\n",
    "y_probas = reconstruct([probas_full, probas_nan], [idxt_full, idxt_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "loss = metrics.log_loss(y_test, y_probas[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Confusion matrix\")\n",
    "print(tabulate(confusion, headers=['0', '1'], tablefmt=\"fancy_grid\"))\n",
    "print(\"Log-loss: {:0.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_ID = X_test.ID.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xt_full, Xt_nan, _, _, idxt_full, idxt_nan = preprocess(X_test, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_full = pipe_full.predict_proba(Xt_full)\n",
    "probas_nan = pipe_nan.predict_proba(Xt_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probas = reconstruct([probas_full, probas_nan], [idxt_full, idxt_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_probas_df = pd.DataFrame({'ID': X_test_ID, 'PredictedProb': y_probas[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_probas_df.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
